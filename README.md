# Knowledge Distillation
The goal of this project was to learn about the tools and technologies required to transfer knowledge from a larger model to a smaller one that can be used practically in real-world settings. Specifically, the project focused on the setting of Knowledge distillation as a model compression technique. The project is divided into 2 tasks:

* Task 1: using the conventional knowledge distillation framework as a model compression method for a popular digit classification dataset, “MNIST”  and 
* Task 2: using transfer learning and knowledge distillation to train a lightweight model for mimicking a pre-trained larger model in a clinical histopathology dataset, “MHIST” . 
